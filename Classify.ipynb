{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports (pip install everything if you havent already)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only run if you need to install the packages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install sklearn\n",
    "!python3 -m pip install numpy\n",
    "!python3 -m pip install pandas\n",
    "!python3 -m pip install matplotlib\n",
    "!python3 -m pip install seaborn\n",
    "!python3 -m pip install beautifulsoup4\n",
    "!python3 -m pip install newspaper3k\n",
    "!python3 -m pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run this for the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from newspaper import Article\n",
    "from sklearn.metrics  import f1_score,accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data in a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "don't run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# get json objects from /jsons folder into a pandas dataframe\n",
    "\n",
    "jsons = pd.DataFrame(columns=[\"topic\", \"source\", \"bias\", \"url\", \"title\", \"date\", \"authors\", \"content\", \"content_original\", \"source_url\", \"bias_text\", \"ID\"])\n",
    "for file in os.listdir('jsons'):\n",
    "    if file.endswith('.json'):\n",
    "        with open(os.path.join('jsons', file)) as json_file:\n",
    "            # get json object as dictionary\n",
    "            data = json.load(json_file)\n",
    "            # get values from dictionary in an array\n",
    "            values = list(data.values())\n",
    "            # put the values in a new row in the dataframe\n",
    "            jsons.loc[len(jsons)] = values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsons['processed_content'] = articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe to csv\n",
    "jsons.to_csv('processed_articles.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataframe and preprocess the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsons = pd.read_csv('processed_articles.csv')\n",
    "articles = jsons['processed_content'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "# Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "# Removing URL's\n",
    "def remove_urls(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "def remove_stopwords(text, language):\n",
    "    text = text.lower()\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    word_tokens = text.split()\n",
    "    return \" \".join([word for word in word_tokens if word not in stop_words])\n",
    "\n",
    "# Final function to clean the text\n",
    "def clean_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    text = remove_urls(text)\n",
    "    text = remove_stopwords(text, \"english\")\n",
    "    return text\n",
    "    \n",
    "articles = [clean_text(article) for article in articles]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use count vectorization to vector representation of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "\n",
    "x = cv.fit_transform(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only run one of these"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighbor Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from copy import copy\n",
    "\n",
    "# parameters of classifier\n",
    "k = 3\n",
    "\n",
    "y_feat = 'bias_text'\n",
    "\n",
    "y_true = jsons.loc[:, y_feat].values\n",
    "\n",
    "# initialize a knn_classifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "# construction of kfold object\n",
    "kfold = KFold(n_splits=4)\n",
    "\n",
    "# allocate an empty array to store predictions in\n",
    "y_pred = copy(y_true)\n",
    "\n",
    "for train_idx, test_idx in kfold.split(x, y_true):\n",
    "    # build arrays which correspond to x, y train /test\n",
    "    x_test = x[test_idx, :]\n",
    "    x_train = x[train_idx, :]\n",
    "    y_true_train = y_true[train_idx]\n",
    "    \n",
    "    # fit happens \"inplace\", we modify the internal state of knn_classifier to remember all the training samples\n",
    "    knn_classifier.fit(x_train, y_true_train)\n",
    "\n",
    "    # estimate each penguin's species\n",
    "    y_pred[test_idx] = knn_classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "n_splits = 5\n",
    "max_depth = 7\n",
    "\n",
    "y_feat = 'bias_text'\n",
    "y = jsons.loc[:, y_feat].values\n",
    "\n",
    "skfold = StratifiedKFold(n_splits=n_splits)\n",
    "\n",
    "y_pred = np.empty_like(y)\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=10)\n",
    "\n",
    "for train_idx, test_idx in skfold.split(x, y):\n",
    "    x_train = x[train_idx, :]\n",
    "    y_train = y[train_idx]\n",
    "\n",
    "    x_test = x[test_idx, :]\n",
    "\n",
    "    rf_clf = rf_clf.fit(x_train, y_train)\n",
    "\n",
    "    y_pred[test_idx] = rf_clf.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "n_splits = 3\n",
    "\n",
    "y_feat = 'bias_text'\n",
    "y = jsons.loc[:, y_feat].values\n",
    "\n",
    "skfold = StratifiedKFold(n_splits=n_splits)\n",
    "\n",
    "y_pred = np.empty_like(y)\n",
    "\n",
    "clf = svm.SVC()\n",
    "\n",
    "for train_idx, test_idx in skfold.split(x, y):\n",
    "    x_train = x[train_idx, :]\n",
    "    y_train = y[train_idx]\n",
    "\n",
    "    x_test = x[test_idx, :]\n",
    "\n",
    "    clf = clf.fit(x_train, y_train)\n",
    "\n",
    "    y_pred[test_idx] = clf.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run after running one of the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'confusion_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\pbido\\Downloads\\ramp-fe-challenge\\news-site\\Classify.ipynb Cell 27\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pbido/Downloads/ramp-fe-challenge/news-site/Classify.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# build and plot confusion matrix\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/pbido/Downloads/ramp-fe-challenge/news-site/Classify.ipynb#X35sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m conf_mat \u001b[39m=\u001b[39m confusion_matrix(y_true\u001b[39m=\u001b[39my, y_pred\u001b[39m=\u001b[39my_pred)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pbido/Downloads/ramp-fe-challenge/news-site/Classify.ipynb#X35sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m conf_mat_disp \u001b[39m=\u001b[39m ConfusionMatrixDisplay(conf_mat, display_labels\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39munique(y))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pbido/Downloads/ramp-fe-challenge/news-site/Classify.ipynb#X35sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m sns\u001b[39m.\u001b[39mset(font_scale\u001b[39m=\u001b[39m\u001b[39m1.5\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'confusion_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "# build and plot confusion matrix\n",
    "conf_mat = confusion_matrix(y_true=y, y_pred=y_pred)\n",
    "conf_mat_disp = ConfusionMatrixDisplay(conf_mat, display_labels=np.unique(y))\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "conf_mat_disp.plot()\n",
    "plt.suptitle('7 Layer Forest Classifier')\n",
    "plt.gcf().set_size_inches(6, 6)\n",
    "plt.grid(False)\n",
    "\n",
    "# change font size of predicted label\n",
    "plt.gca().set_xticklabels(np.unique(y_true), fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5256963306172445"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true=y, y_pred=y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
